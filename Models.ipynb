{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#change runtime to A100 GPU, then run cell:\n",
        "!pip install numpy\n",
        "from numpy.random import seed #fix random seed\n",
        "import os, glob\n",
        "seed(123)"
      ],
      "metadata": {
        "id": "6b-Dx877FCtz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e802492f-57c4-4fbd-e1e1-340658b69cd1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Load data\n",
        "* Load data into two folders: one for split stems (2-stem) and one for effects (4-stem)\n",
        "* Mount to google drive"
      ],
      "metadata": {
        "id": "Pmo9DattC0r_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "DhCiLvpuDT1C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7cfaa04-8b18-4d4e-e049-97948b2781b0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Models- Spleeter\n",
        "\n"
      ],
      "metadata": {
        "id": "XRfXmLlhEKEx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#spleeter needs oldeer version of python, use pyenv to downnload it\n",
        "!git clone https://github.com/pyenv/pyenv.git ~/.pyenv\n",
        "import os\n",
        "\n",
        "# Set the root for pyenv\n",
        "os.environ['PYENV_ROOT'] = os.path.expanduser(\"~/.pyenv\")\n",
        "# Prepend pyenv's bin folder to PATH\n",
        "os.environ['PATH'] = os.environ['PYENV_ROOT'] + '/bin:' + os.environ['PATH']\n",
        "\n",
        "# Confirm pyenv is callable\n",
        "!pyenv --version\n",
        "!apt-get install libffi-dev\n",
        "!pyenv install 3.8.13\n",
        "!pyenv global 3.8.13\n",
        "!pyenv exec pip install spleeter\n",
        "!pyenv exec spleeter"
      ],
      "metadata": {
        "id": "AHzw6uOkDvoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2-stem separation"
      ],
      "metadata": {
        "id": "U88qAP5uCgyW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "pyenv exec python - << 'EOF'\n",
        "from spleeter.separator import Separator\n",
        "import os, glob\n",
        "\n",
        "dataset_2 =  \"/content/drive/My Drive/MIR_Project/Paired_Stems\"\n",
        "data_home = dataset_2\n",
        "output_home = \"/content/drive/My Drive/MIR_Project/Spleeter_Outputs/Paired_Stems\"\n",
        "os.makedirs(output_home, exist_ok=True)\n",
        "\n",
        "# load model\n",
        "separator = Separator(\"spleeter:2stems\")\n",
        "\n",
        "# get audio files\n",
        "audio_files = glob.glob(os.path.join(data_home, \"**/*.wav\"), recursive=True)\n",
        "print(f\"Found {len(audio_files)} files.\")\n",
        "\n",
        "# run model\n",
        "for audio_path in audio_files:\n",
        "    try:\n",
        "        print(\"Separating:\", audio_path)\n",
        "        song_folder = os.path.basename(os.path.dirname(audio_path))\n",
        "\n",
        "        song_out_dir = os.path.join(output_home, song_folder)\n",
        "        os.makedirs(song_out_dir, exist_ok=True)\n",
        "\n",
        "        separator.separate_to_file(audio_path, song_out_dir)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"Failed on\", audio_path, \"->\", e)\n",
        "EOF"
      ],
      "metadata": {
        "id": "9iRL0s04EoXY",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4-stem separation (run original files along with processed)"
      ],
      "metadata": {
        "id": "JAMGXPRQCkAW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "pyenv exec python - << 'EOF'\n",
        "from spleeter.separator import Separator\n",
        "import os, glob\n",
        "\n",
        "dataset_4 = \"/content/drive/My Drive/MIR_Project/Effects\"\n",
        "data_home = dataset_4\n",
        "output_home = \"/content/drive/My Drive/MIR_Project/Spleeter_Outputs/Effects\"\n",
        "os.makedirs(output_home, exist_ok=True)\n",
        "\n",
        "# load model\n",
        "separator = Separator(\"spleeter:4stems\")\n",
        "\n",
        "# get audio files\n",
        "audio_files = glob.glob(os.path.join(data_home, \"**/*.wav\"), recursive=True)\n",
        "print(f\"Found {len(audio_files)} files for 4-stem separation.\")\n",
        "\n",
        "# run model\n",
        "for audio_path in audio_files:\n",
        "    try:\n",
        "        print(\"Separating (4 stems):\", audio_path)\n",
        "        separator.separate_to_file(audio_path, output_home)\n",
        "    except Exception as e:\n",
        "        print(\"Failed on\", audio_path, \"->\", e)\n",
        "EOF"
      ],
      "metadata": {
        "id": "7xZjgFT3CImz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Models- Hybrid Demucs\n"
      ],
      "metadata": {
        "id": "qikgU5zBOal6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torchcodec\n",
        "import torch, torchaudio\n",
        "from torchaudio.pipelines import HDEMUCS_HIGH_MUSDB_PLUS\n",
        "\n",
        "#pick whether to run model on GPU or CPU depending on availability.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"device:\", device)\n"
      ],
      "metadata": {
        "id": "Ejtr0dSzOcPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2-stems"
      ],
      "metadata": {
        "id": "J6R1FRXcFRgf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, glob\n",
        "import torchaudio\n",
        "import torch\n",
        "\n",
        "# paths\n",
        "dataset_2 = \"/content/drive/My Drive/MIR_Project/Paired_Stems\"\n",
        "data_home = dataset_2\n",
        "output_home  = \"/content/drive/My Drive/MIR_Project/Demucs_Outputs/Paired_Stems\"\n",
        "os.makedirs(output_home, exist_ok=True)\n",
        "\n",
        "# load model\n",
        "bundle = HDEMUCS_HIGH_MUSDB_PLUS\n",
        "model = bundle.get_model().to(device).eval()\n",
        "labels = [\"drums\", \"bass\", \"other\", \"vocals\"]\n",
        "sr_target = bundle.sample_rate\n",
        "\n",
        "audio_files = glob.glob(os.path.join(data_home, \"**/*.wav\"), recursive=True)\n",
        "print(f\"Found {len(audio_files)} .wav files.\")\n",
        "\n",
        "processed_count_demucs_2stems = 0\n",
        "\n",
        "# run model\n",
        "for audio_path in audio_files:\n",
        "    try:\n",
        "        print(\"Separating:\", audio_path)\n",
        "\n",
        "        waveform, sr = torchaudio.load(audio_path)\n",
        "\n",
        "        if sr != sr_target:\n",
        "            waveform = torchaudio.functional.resample(waveform, sr, sr_target)\n",
        "\n",
        "        if waveform.size(0) == 1:\n",
        "            waveform = waveform.repeat(2, 1)\n",
        "\n",
        "        with torch.inference_mode():\n",
        "            sources4 = model(waveform.unsqueeze(0).to(device))\n",
        "\n",
        "        if sources4.dim() == 4:\n",
        "            sources4 = sources4[0]\n",
        "\n",
        "        drums  = sources4[0]\n",
        "        bass   = sources4[1]\n",
        "        other  = sources4[2]\n",
        "        vocals = sources4[3]\n",
        "\n",
        "        # Merge to 2 stems\n",
        "        accompaniment = drums + bass + other\n",
        "        sources2 = torch.stack([accompaniment, vocals], dim=0)\n",
        "        labels2  = [\"accompaniment\", \"vocals\"]\n",
        "\n",
        "        #Save\n",
        "        track = os.path.splitext(os.path.basename(audio_path))[0]\n",
        "        track_dir = os.path.join(output_home, track)\n",
        "        os.makedirs(track_dir, exist_ok=True)\n",
        "\n",
        "        for i, name in enumerate(labels2):\n",
        "            torchaudio.save(\n",
        "                os.path.join(track_dir, f\"{name}.wav\"),\n",
        "                sources2[i].cpu(),\n",
        "                sr_target\n",
        "            )\n",
        "\n",
        "        processed_count_demucs_2stems += 1\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"Failed on\", audio_path, \"->\", e)\n",
        "\n",
        "print(f\"Successfully processed {processed_count_demucs_2stems} audio files for Demucs 2-stems separation.\")"
      ],
      "metadata": {
        "id": "BDXJbPbNFQjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4-stems (run original files along with processed)"
      ],
      "metadata": {
        "id": "FUte_oNxFS_J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#paths\n",
        "dataset_4 = \"/content/drive/My Drive/MIR_Project/Effects\" #run again with original\n",
        "data_home = dataset_4\n",
        "output_home  = \"/content/drive/My Drive/MIR_Project/Demucs_Outputs/Effects\" #run again with original\n",
        "os.makedirs(output_home, exist_ok=True)\n",
        "\n",
        "#load model\n",
        "bundle = HDEMUCS_HIGH_MUSDB_PLUS\n",
        "model = bundle.get_model().to(device).eval()\n",
        "labels = [\"drums\", \"bass\", \"other\", \"vocals\"]\n",
        "sr_target = bundle.sample_rate      # 44100\n",
        "\n",
        "#get audio files\n",
        "audio_files = glob.glob(os.path.join(data_home, \"**/*.wav\"), recursive=True)\n",
        "print(f\"Found {len(audio_files)} files.\")\n",
        "\n",
        "processed_count_demucs_4stems = 0\n",
        "\n",
        "# run model\n",
        "for audio_path in audio_files:\n",
        "    try:\n",
        "        print(\"Separating:\", audio_path)\n",
        "\n",
        "        waveform, sr = torchaudio.load(audio_path)\n",
        "\n",
        "        if sr != sr_target:\n",
        "            waveform = torchaudio.functional.resample(waveform, sr, sr_target)\n",
        "\n",
        "        if waveform.size(0) == 1:\n",
        "            waveform = waveform.repeat(2, 1)\n",
        "\n",
        "        with torch.inference_mode():\n",
        "            # Add a batch dimension here: [channels, frames] -> [1, channels, frames]\n",
        "            sources = model(waveform.unsqueeze(0).to(device))  # [4, channels, time]\n",
        "            #if using chunking, replace with:\n",
        "            # sources = separate_sources(model, waveform[None].to(device))[0]\n",
        "\n",
        "        track = os.path.splitext(os.path.basename(audio_path))[0]\n",
        "        track_dir = os.path.join(output_home, track)\n",
        "        os.makedirs(track_dir, exist_ok=True)\n",
        "\n",
        "        for i, name in enumerate(labels):\n",
        "            torchaudio.save(\n",
        "                os.path.join(track_dir, f\"{name}.wav\"),\n",
        "                sources[0][i].cpu(), # Access the first (and only) item in the batch\n",
        "                sr_target\n",
        "            )\n",
        "        processed_count_demucs_4stems += 1\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"Failed on\", audio_path, \"->\", e)\n",
        "print(f\"Successfully processed {processed_count_demucs_4stems} audio files for Demucs 4-stems separation.\")\n",
        "\n",
        "#''''\n",
        "#***If that ^ takes up too much memory, then do this instead (chunking)*****\n",
        "\n",
        "#from torchaudio.transforms import Fade\n",
        "#import torch\n",
        "\n",
        "#def separate_sources(model, mix, segment=10.0, overlap=0.1, device=None):\n",
        "#    if device is None:\n",
        "#        device = mix.device\n",
        "#    else:\n",
        "#        device = torch.device(device)\n",
        "\n",
        "#    batch, channels, length = mix.shape\n",
        "#    sample_rate = 44100  # bundle.sample_rate\n",
        "#    chunk_len = int(sample_rate * segment * (1 + overlap))\n",
        "#    start = 0\n",
        "#    end = chunk_len\n",
        "#    overlap_frames = int(overlap * sample_rate)\n",
        "#    fade = Fade(fade_in_len=0, fade_out_len=overlap_frames, fade_shape=\"linear\")\n",
        "\n",
        "#    final = torch.zeros(batch, len(model.sources), channels, length, device=device)\n",
        "\n",
        "#    while start < length - overlap_frames:\n",
        "#        chunk = mix[:, :, start:end]\n",
        "#        with torch.no_grad():\n",
        "#            out = model(chunk)\n",
        "#        out = fade(out)\n",
        "#        final[:, :, :, start:end] += out\n",
        "\n",
        "#        start += chunk_len - overlap_frames\n",
        "#        end = start + chunk_len\n",
        "\n",
        "#        if end >= length:\n",
        "#            end = length\n",
        "#            fade.fade_out_len = 0\n",
        "\n",
        "#    return final\n",
        "\n",
        "#''''''"
      ],
      "metadata": {
        "id": "_y-geazsZRcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3 Models- Evaluation"
      ],
      "metadata": {
        "id": "okcXemnbeePV"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZAemm02sejxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3.1 Experiment 1"
      ],
      "metadata": {
        "id": "YjaZqdYPelHx"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uQ2cMcqNenYj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}